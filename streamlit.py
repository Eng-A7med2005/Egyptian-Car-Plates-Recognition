import os
os.system("apt-get update && apt-get install -y libgl1")

import streamlit as st
import torch
import numpy as np
import gdown
import cv2
from PIL import Image
from ultralytics import YOLO

# Ø±ÙˆØ§Ø¨Ø· Google Drive
plate_model_url = 'https://drive.google.com/uc?id=12tRfc_-nOkqMO9bdwpV8P8MFamwgtR2e'
ocr_model_url = 'https://drive.google.com/uc?id=YOUR_OCR_MODEL_ID'  # â† Ø¶Ø¹ Ù‡Ù†Ø§ ID Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø«Ø§Ù†ÙŠ

# Ù…Ø³Ø§Ø±Ø§Øª Ø­ÙØ¸ Ø§Ù„Ù…Ù„ÙØ§Øª
plate_model_path = 'yolo11m_car_plate_trained.pt'
ocr_model_path = 'yolo11m_car_plate_ocr1.pt'

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
gdown.download(plate_model_url, plate_model_path, quiet=False)
gdown.download(ocr_model_url, ocr_model_path, quiet=False)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª
plate_detector = YOLO(plate_model_path)
ocr_model = YOLO(ocr_model_path)

# Ù‚Ø§Ù…ÙˆØ³ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ
char_map = {
    'alef': 'Ø§', 'baa': 'Ø¨', 'taa': 'Øª', 'thaa': 'Ø«', 'jeem': 'Ø¬', 'haa': 'Ø­', 'khaa': 'Ø®',
    'dal': 'Ø¯', 'dhal': 'Ø°', 'raa': 'Ø±', 'zay': 'Ø²', 'seen': 'Ø³', 'sheen': 'Ø´', 'saad': 'Øµ',
    'daad': 'Ø¶', 'taa2': 'Ø·', 'thaa2': 'Ø¸', 'ain': 'Ø¹', 'ghain': 'Øº', 'fa': 'Ù', 'qaf': 'Ù‚',
    'kaf': 'Ùƒ', 'lam': 'Ù„', 'meem': 'Ù…', 'noon': 'Ù†', 'ha': 'Ù‡', 'waw': 'Ùˆ', 'yaa': 'ÙŠ'
}

number_map = {
    '0': 'Ù ', '1': 'Ù¡', '2': 'Ù¢', '3': 'Ù£', '4': 'Ù¤',
    '5': 'Ù¥', '6': 'Ù¦', '7': 'Ù§', '8': 'Ù¨', '9': 'Ù©'
}

# ÙˆØ§Ø¬Ù‡Ø© Streamlit
st.title("ğŸš˜ Egyptian Car Plate Recognition")

uploaded_image = st.file_uploader("Upload a Car Image", type=["jpg", "jpeg", "png"])

if uploaded_image is not None:
    file_bytes = np.frombuffer(uploaded_image.read(), np.uint8)
    image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    st.image(rgb_image, caption="Uploaded Image", use_column_width=True)

    # ÙƒØ´Ù Ø§Ù„Ù„ÙˆØ­Ø§Øª
    plate_results = plate_detector.predict(rgb_image, conf=0.3)

    if len(plate_results[0].boxes) == 0:
        st.warning("Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£ÙŠ Ù„ÙˆØ­Ø©.")
    else:
        st.subheader("Ø§Ù„Ù„ÙˆØ­Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… Ø§ÙƒØªØ´Ø§ÙÙ‡Ø§:")
        for i, box in enumerate(plate_results[0].boxes):
            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
            plate_crop = rgb_image[y1:y2, x1:x2]

            # Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø­Ø±Ù
            ocr_results = ocr_model.predict(plate_crop, conf=0.25)
            plotted = ocr_results[0].plot()

            detected_items = []
            for char_box in ocr_results[0].boxes:
                class_id = int(char_box.cls[0])
                name = ocr_results[0].names[class_id]
                x_center = (char_box.xyxy[0][0] + char_box.xyxy[0][2]) / 2
                detected_items.append((x_center, name))

            detected_items.sort(key=lambda x: x[0])

            plate_text = ''.join([char_map.get(item[1], item[1]) for item in detected_items])
            plate_text = ''.join([number_map.get(char, char) for char in plate_text])
            plate_text = ' '.join(reversed(plate_text))

            st.image(plate_crop, caption=f"Plate #{i+1}", use_column_width=True)
            st.image(plotted, caption=f"Characters Detected", use_column_width=True)
            st.success(f"ğŸ”  Recognized Text: **{plate_text}**")

    st.image(plate_results[0].plot(), caption="Image with Detected Plates", use_column_width=True)
