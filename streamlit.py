import streamlit as st
import cv2
import torch
import numpy as np
from ultralytics import YOLO

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
plate_model_path = 'yolo11m_car_plate_trained.pt'
ocr_model_path = 'yolo11m_car_plate_ocr1.pt'

plate_detector = YOLO(plate_model_path)
ocr_model = YOLO(ocr_model_path)

# Ù‚Ø§Ù…ÙˆØ³ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù„Ø§ØªÙŠÙ†ÙŠØ© Ø¥Ù„Ù‰ Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÙŠØ©
char_map = {
    'meem': 'Ù…',  # meem -> Ù…
    'yaa': 'ÙŠ',   # yaa -> ÙŠ
    'alef': 'Ø§',  # alef -> Ø§
    'baa': 'Ø¨',   # baa -> Ø¨
    'taa': 'Øª',   # taa -> Øª
    'thaa': 'Ø«',  # thaa -> Ø«
    'jeem': 'Ø¬',  # jeem -> Ø¬
    'haa': 'Ø­',   # haa -> Ø­
    'khaa': 'Ø®',  # khaa -> Ø®
    'daal': 'Ø¯',   # dal -> Ø¯
    'dhal': 'Ø°',  # dhal -> Ø°
    'raa': 'Ø±',   # raa -> Ø±
    'zay': 'Ø²',   # zay -> Ø²
    'seen': 'Ø³',  # seen -> Ø³
    'sheen': 'Ø´', # sheen -> Ø´
    'saad': 'Øµ',  # saad -> Øµ
    'daad': 'Ø¶',  # daad -> Ø¶
    'taa': 'Ø·',   # taa -> Ø·
    'thaa': 'Ø¸',  # thaa -> Ø¸
    'ain': 'Ø¹',   # ain -> Ø¹
    'ghain': 'Øº', # ghain -> Øº
    'faa': 'Ù',    # fa -> Ù
    'qaaf': 'Ù‚',   # qaf -> Ù‚
    'kaf': 'Ùƒ',   # kaf -> Ùƒ
    'laam': 'Ù„',   # lam -> Ù„
    'meem': 'Ù…',  # meem -> Ù…
    'noon': 'Ù†',  # noon -> Ù†
    'haa': 'Ù‡',    # ha -> Ù‡
    'waaw': 'Ùˆ',   # waw -> Ùˆ
    'yaa': 'ÙŠ'    # yaa -> ÙŠ
}

# Ù‚Ø§Ù…ÙˆØ³ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù„Ø§ØªÙŠÙ†ÙŠØ© Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… Ø¹Ø±Ø¨ÙŠØ©
number_map = {
    '0': 'Ù ',
    '1': 'Ù¡',
    '2': 'Ù¢',
    '3': 'Ù£',
    '4': 'Ù¤',
    '5': 'Ù¥',
    '6': 'Ù¦',
    '7': 'Ù§',
    '8': 'Ù¨',
    '9': 'Ù©',
}

# ÙˆØ§Ø¬Ù‡Ø© Streamlit
st.markdown(
    """
    <h1 style='text-align: center;'>ğŸš˜ Car Plate Recognition Using Two YOLOv8 Models</h1>
    <h3 style='text-align: center;'>By AIvolution Team</h3>
    """,
    unsafe_allow_html=True
)

uploaded_image = st.file_uploader("Upload a Car Image", type=["jpg", "jpeg", "png"])

if uploaded_image is not None:
    file_bytes = np.frombuffer(uploaded_image.read(), np.uint8)
    image = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    st.image(rgb_image, caption="Uploaded Image", use_column_width=True)

    # --- Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ù„ÙˆØ­Ø© ---
    plate_results = plate_detector.predict(rgb_image, conf=0.3)

    if len(plate_results[0].boxes) == 0:
        st.warning("No plates were detected.")
    else:
        st.subheader("Detected Plates and Their Content:")
        for i, box in enumerate(plate_results[0].boxes):
            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
            plate_crop = rgb_image[y1:y2, x1:x2]

            # --- Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø«Ø§Ù†ÙŠ ---
            ocr_results = ocr_model.predict(plate_crop, conf=0.25)
            plotted = ocr_results[0].plot()

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ø±ÙˆÙ ÙˆØ§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¨ØªØ±ØªÙŠØ¨Ù‡Ù…
            detected_items = []
            for char_box in ocr_results[0].boxes:
                class_id = int(char_box.cls[0])
                name = ocr_results[0].names[class_id]
                x_center = (char_box.xyxy[0][0] + char_box.xyxy[0][2]) / 2
                detected_items.append((x_center, name))

            detected_items.sort(key=lambda x: x[0])

            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù„Ø§ØªÙŠÙ†ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            plate_text = ''.join([char_map.get(item[1], item[1]) for item in detected_items])

            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… Ø¹Ø±Ø¨ÙŠØ©
            plate_text = ''.join([number_map.get(char, char) for char in plate_text])

            # Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§ÙØ© Ø¨ÙŠÙ† ÙƒÙ„ Ø­Ø±Ù ÙˆØ±Ù‚Ù…
            plate_text = ' '.join([char for char in plate_text])

            # Ø¹ÙƒØ³ Ø§Ù„Ù†Øµ Ø¨Ø­ÙŠØ« ÙŠØ¸Ù‡Ø± Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
            plate_text = ' '.join(reversed(plate_text.split()))

            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            st.image(plate_crop, caption=f"Plate Region #{i+1}", use_column_width=True)
            st.image(plotted, caption=f"Characters Detected in Plate #{i+1}", use_column_width=True)
            st.success(f"ğŸ”  Recognized Text: **{plate_text}**") 

    # Ø¹Ø±Ø¶ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª
    st.image(plate_results[0].plot(), caption="Image with Detected Plates", use_column_width=True)
